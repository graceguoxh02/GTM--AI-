import {
  GTM_NAVIGATOR_SYSTEM_PROMPT,
  POSITIONING_ANALYSIS_PROMPT,
  MARKET_ANALYSIS_PROMPT,
  USER_ANALYSIS_PROMPT,
  SALES_ANALYSIS_PROMPT,
  GTM_ROADMAP_PROMPT,
  CANVAS_ANALYSIS_PROMPT,
  COMPETITOR_ANALYSIS_PROMPT,
  DOCUMENT_SUMMARY_PROMPT
} from './prompts';

import {
  MODEL_PROVIDERS,
  SCENARIO_TYPES,
  selectOptimalModel,
  getFallbackModel
} from './modelConfig';

// ============================================
// AI æœåŠ¡å±‚ - æ”¯æŒå¤šæ¨¡å‹æ™ºèƒ½è·¯ç”±
// ============================================

/**
 * é€šç”¨çš„ AI API è°ƒç”¨ï¼ˆé€šè¿‡åç«¯ä»£ç†ï¼‰
 * æ”¯æŒï¼šé€šä¹‰åƒé—®ã€DeepSeekã€æ™ºè°± GLMã€Kimi
 * âš ï¸ å®‰å…¨ï¼šAPI Keys ä¿å­˜åœ¨åç«¯ï¼Œå‰ç«¯æ— æ³•è®¿é—®
 */
async function callOpenAICompatibleAPI(provider, model, messages, systemPrompt, maxTokens = 2000) {
  const providerConfig = MODEL_PROVIDERS[provider];
  const modelConfig = providerConfig.models[model];

  if (!providerConfig || !modelConfig) {
    throw new Error(`ä¸æ”¯æŒçš„æ¨¡å‹: ${provider}/${model}`);
  }

  // æ„å»ºè¯·æ±‚ï¼ˆå‘é€åˆ°åç«¯ APIï¼‰
  const requestBody = {
    provider,
    model,
    messages: [
      { role: 'system', content: systemPrompt },
      ...messages.map(msg => ({
        role: msg.role === 'assistant' ? 'assistant' : 'user',
        content: msg.content
      }))
    ],
    maxTokens,
    temperature: 0.7
  };

  // è°ƒç”¨åç«¯ APIï¼ˆæœ¬åœ°å¼€å‘æ—¶ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œéƒ¨ç½²åè‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„åŸŸåï¼‰
  const apiUrl = '/api/chat';

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(requestBody)
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: 'Unknown error' }));
    throw new Error(error.error || `API è°ƒç”¨å¤±è´¥: ${response.status}`);
  }

  const data = await response.json();
  return {
    success: true,
    content: data.choices[0].message.content,
    model: `${providerConfig.name}/${modelConfig.name}`,
    usage: data.usage
  };
}

// æ³¨æ„ï¼šcallGLMAPI å‡½æ•°å·²ç§»é™¤ï¼Œç°åœ¨æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€é€šè¿‡åç«¯ API å¤„ç†

/**
 * æµå¼è°ƒç”¨ï¼ˆé€šè¿‡åç«¯ä»£ç†ï¼Œæ”¯æŒ SSEï¼‰
 * âš ï¸ å®‰å…¨ï¼šAPI Keys ä¿å­˜åœ¨åç«¯ï¼Œå‰ç«¯æ— æ³•è®¿é—®
 */
async function streamOpenAICompatibleAPI(provider, model, messages, systemPrompt, onToken, onComplete) {
  const providerConfig = MODEL_PROVIDERS[provider];
  const modelConfig = providerConfig.models[model];

  if (!providerConfig || !modelConfig) {
    throw new Error(`ä¸æ”¯æŒçš„æ¨¡å‹: ${provider}/${model}`);
  }

  const requestBody = {
    provider,
    model,
    messages: [
      { role: 'system', content: systemPrompt },
      ...messages.map(msg => ({
        role: msg.role === 'assistant' ? 'assistant' : 'user',
        content: msg.content
      }))
    ],
    maxTokens: 2000,
    temperature: 0.7
  };

  // è°ƒç”¨åç«¯ API
  const response = await fetch('/api/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(requestBody)
  });

  if (!response.ok) {
    throw new Error(`æµå¼è°ƒç”¨å¤±è´¥: ${response.status}`);
  }

  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let fullText = '';

  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n').filter(line => line.trim() !== '');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          if (data === '[DONE]') continue;

          try {
            const json = JSON.parse(data);
            const content = json.choices[0]?.delta?.content;
            if (content) {
              fullText += content;
              onToken(content, fullText);
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    }

    onComplete(fullText);
    return { success: true, content: fullText };
  } catch (error) {
    console.error('æµå¼å¤„ç†é”™è¯¯:', error);
    throw error;
  }
}

/**
 * æ™ºèƒ½ AI è°ƒç”¨ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
 * @param {string} scenario - åœºæ™¯ç±»å‹
 * @param {string} systemPrompt - ç³»ç»Ÿæç¤ºè¯
 * @param {Array} messages - å¯¹è¯å†å²
 * @param {boolean} hasImage - æ˜¯å¦åŒ…å«å›¾ç‰‡
 * @param {boolean} isLongText - æ˜¯å¦é•¿æ–‡æœ¬
 * @param {number} maxTokens - æœ€å¤§ token æ•°
 */
export async function smartChatWithAI(scenario, systemPrompt, messages, hasImage = false, isLongText = false, maxTokens = 2000) {
  try {
    // æ™ºèƒ½é€‰æ‹©æ¨¡å‹
    const { provider, model } = selectOptimalModel(scenario, hasImage, isLongText);

    console.log(`ğŸ¤– åœºæ™¯: ${scenario}, é€‰æ‹©æ¨¡å‹: ${provider}/${model}`);

    // è°ƒç”¨ä¸»æ¨¡å‹
    try {
      return await callOpenAICompatibleAPI(provider, model, messages, systemPrompt, maxTokens);
    } catch (error) {
      console.warn(`ä¸»æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œå°è¯•é™çº§...`, error);

      // é™çº§åˆ°å¤‡ç”¨æ¨¡å‹
      const fallback = getFallbackModel(provider, model);
      console.log(`âš ï¸ é™çº§åˆ°: ${fallback.provider}/${fallback.model}`);

      return await callOpenAICompatibleAPI(fallback.provider, fallback.model, messages, systemPrompt, maxTokens);
    }
  } catch (error) {
    console.error('AI è°ƒç”¨å¤±è´¥:', error);
    return {
      success: false,
      error: error.message || 'è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API é…ç½®'
    };
  }
}

/**
 * æ™ºèƒ½æµå¼è°ƒç”¨
 */
export async function smartStreamChatWithAI(scenario, systemPrompt, messages, onToken, onComplete, hasImage = false) {
  try {
    const { provider, model } = selectOptimalModel(scenario, hasImage);

    console.log(`ğŸ¤– æµå¼åœºæ™¯: ${scenario}, é€‰æ‹©æ¨¡å‹: ${provider}/${model}`);

    return await streamOpenAICompatibleAPI(provider, model, messages, systemPrompt, onToken, onComplete);
  } catch (error) {
    console.error('æµå¼è°ƒç”¨å¤±è´¥:', error);
    return {
      success: false,
      error: error.message || 'æµå¼è°ƒç”¨å¤±è´¥'
    };
  }
}

// ============================================
// å„åœºæ™¯ä¸“ç”¨çš„ AI è°ƒç”¨å‡½æ•°
// ============================================

/**
 * GTM å¼•å¯¼å¸ˆå¯¹è¯
 */
export const chatWithNavigator = async (messages, hasImage = false) => {
  return smartChatWithAI(
    SCENARIO_TYPES.CHAT,
    GTM_NAVIGATOR_SYSTEM_PROMPT,
    messages,
    hasImage
  );
};

/**
 * GTM å¼•å¯¼å¸ˆå¯¹è¯ - æµå¼ç‰ˆæœ¬
 */
export const streamChatWithNavigator = async (messages, onToken, onComplete, hasImage = false) => {
  return smartStreamChatWithAI(
    SCENARIO_TYPES.CHAT,
    GTM_NAVIGATOR_SYSTEM_PROMPT,
    messages,
    onToken,
    onComplete,
    hasImage
  );
};

/**
 * äº§å“å®šä½åˆ†æ
 */
export const analyzePositioning = async (productInfo, hasImage = false) => {
  const prompt = POSITIONING_ANALYSIS_PROMPT(productInfo);
  return smartChatWithAI(
    SCENARIO_TYPES.ANALYSIS,
    'ä½ æ˜¯ä¸€ä½äº§å“å®šä½ä¸“å®¶ã€‚',
    [{ role: 'user', content: prompt }],
    hasImage,
    false,
    3000
  );
};

/**
 * å¸‚åœºé€‰æ‹©åˆ†æ
 */
export const analyzeMarket = async (productInfo, hasImage = false) => {
  const prompt = MARKET_ANALYSIS_PROMPT(productInfo);
  return smartChatWithAI(
    SCENARIO_TYPES.ANALYSIS,
    'ä½ æ˜¯ä¸€ä½å¸‚åœºç­–ç•¥ä¸“å®¶ã€‚',
    [{ role: 'user', content: prompt }],
    hasImage,
    false,
    3000
  );
};

/**
 * ç›®æ ‡ç”¨æˆ·åˆ†æ
 */
export const analyzeUsers = async (productInfo, hasImage = false) => {
  const prompt = USER_ANALYSIS_PROMPT(productInfo);
  return smartChatWithAI(
    SCENARIO_TYPES.ANALYSIS,
    'ä½ æ˜¯ä¸€ä½ç”¨æˆ·ç ”ç©¶ä¸“å®¶ã€‚',
    [{ role: 'user', content: prompt }],
    hasImage,
    false,
    3000
  );
};

/**
 * é”€å”®ä¸æ¨å¹¿åˆ†æ
 */
export const analyzeSales = async (productInfo, hasImage = false) => {
  const prompt = SALES_ANALYSIS_PROMPT(productInfo);
  return smartChatWithAI(
    SCENARIO_TYPES.ANALYSIS,
    'ä½ æ˜¯ä¸€ä½å¢é•¿ä¸“å®¶ã€‚',
    [{ role: 'user', content: prompt }],
    hasImage,
    false,
    3000
  );
};

/**
 * GTM è·¯å¾„åˆ†æ
 */
export const analyzeGTMRoadmap = async (productInfo, hasImage = false) => {
  const prompt = GTM_ROADMAP_PROMPT(productInfo);
  return smartChatWithAI(
    SCENARIO_TYPES.ANALYSIS,
    'ä½ æ˜¯ä¸€ä½ SaaS åˆ›ä¸šå¯¼å¸ˆã€‚',
    [{ role: 'user', content: prompt }],
    hasImage,
    false,
    3000
  );
};

/**
 * å•†ä¸šæ¨¡å¼ç”»å¸ƒæŸä¸ªç»´åº¦çš„åˆ†æ
 */
export const analyzeCanvasDimension = async (productInfo, dimension, hasImage = false) => {
  const prompt = CANVAS_ANALYSIS_PROMPT(productInfo, dimension);
  return smartChatWithAI(
    SCENARIO_TYPES.ANALYSIS,
    'ä½ æ˜¯å•†ä¸šæ¨¡å¼ä¸“å®¶ã€‚',
    [{ role: 'user', content: prompt }],
    hasImage,
    false,
    2000
  );
};

/**
 * ç«å“åˆ†æ
 */
export const analyzeCompetitor = async (productInfo, competitorName, hasImage = false) => {
  const prompt = COMPETITOR_ANALYSIS_PROMPT(productInfo, competitorName);
  return smartChatWithAI(
    SCENARIO_TYPES.COMPETITOR,
    'ä½ æ˜¯ç«å“åˆ†æä¸“å®¶ã€‚',
    [{ role: 'user', content: prompt }],
    hasImage,
    false,
    2000
  );
};

/**
 * æ–‡æ¡£è§£æå’Œæ€»ç»“
 */
export const summarizeDocument = async (documentContent, productName) => {
  const prompt = DOCUMENT_SUMMARY_PROMPT(documentContent, productName);
  const isLongText = documentContent.length > 10000;

  return smartChatWithAI(
    SCENARIO_TYPES.DOCUMENT_PARSE,
    'ä½ æ˜¯å•†ä¸šåˆ†æå¸ˆã€‚',
    [{ role: 'user', content: prompt }],
    false,
    isLongText,
    2000
  );
};

/**
 * æ£€æŸ¥ API é…ç½®æ˜¯å¦æ­£ç¡®
 * âš ï¸ æ³¨æ„ï¼šAPI Keys ç°åœ¨ä¿å­˜åœ¨åç«¯ï¼Œå‰ç«¯æ— æ³•ç›´æ¥æ£€æŸ¥
 * é…ç½®æ£€æŸ¥å°†åœ¨å®é™…è°ƒç”¨æ—¶è¿›è¡Œ
 */
export const checkAPIConfig = () => {
  return {
    isConfigured: true,
    message: 'âœ“ API Keys ç”±åç«¯å®‰å…¨ç®¡ç†'
  };
};
